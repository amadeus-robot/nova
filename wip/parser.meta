<function>
  <name>skip_newlines/1</name>
  <old_code>
defp skip_newlines(tokens) do
    Enum.drop_while(tokens, fn %Token{type: t} -> t == :newline end)
  end
  </old_code>
  <description>
Skips leading newline tokens from a list of tokens.
It iterates through the beginning of the token list and removes all tokens whose type is `:newline` until a non-newline token is encountered or the list is exhausted.
  </description>
</function>

<function>
  <name>drop_newlines/1 (first clause)</name>
  <old_code>
defp drop_newlines([%Token{type: :newline} | rest]), do: drop_newlines(rest)
  </old_code>
  <description>
Recursively removes leading newline tokens from a list of tokens. This is the recursive step that handles a leading newline.
  </description>
</function>

<function>
  <name>drop_newlines/1 (second clause)</name>
  <old_code>
defp drop_newlines(tokens), do: tokens
  </old_code>
  <description>
Base case for `drop_newlines/1`. If the token list does not start with a newline, it returns the list unchanged.
  </description>
</function>

<function>
  <name>strip_newlines/1</name>
  <old_code>
defp strip_newlines(list),
    do:
      Enum.reject(list, fn
        %Token{type: :newline} -> true
        _ -> false
      end)
  </old_code>
  <description>
Removes all newline tokens from anywhere within a list of tokens, not just leading ones.
  </description>
</function>

<function>
  <name>ensure_consumed/1</name>
  <old_code>
defp ensure_consumed(rest) do
    case skip_newlines(rest) do
      [] ->
        :ok

      leftover ->
        tok = hd(leftover)

        {:error,
         "unexpected tokens after successful parse – " <>
           "#{tok.type}:#{inspect(tok.value)} at line #{tok.line}, col #{tok.column}"}
    end
  end
  </old_code>
  <description>
Checks if all tokens have been consumed after a parsing operation.
It first skips any trailing newlines. If there are any tokens left, it returns an error indicating unexpected tokens. Otherwise, it returns `:ok`.
This is typically used at the end of a top-level parse to ensure the entire input was processed.
  </description>
</function>

<function>
  <name>parse_declarations/2 (public arity 1)</name>
  <old_code>
def parse_declarations(tokens), do: parse_declarations(tokens, [])
  </old_code>
  <description>
Public entry point for parsing a sequence of declarations. It initializes an empty accumulator for the declarations.
  </description>
</function>

<function>
  <name>parse_declarations/2 (base case)</name>
  <old_code>
def parse_declarations([], acc), do: {:ok, Enum.reverse(acc), []}
  </old_code>
  <description>
Base case for `parse_declarations/2`. If the token list is empty, it returns the accumulated declarations (reversed to maintain order) and an empty list for remaining tokens.
  </description>
</function>

<function>
  <name>parse_declarations/2 (recursive step)</name>
  <old_code>
def parse_declarations(tokens, acc) do
    tokens = skip_newlines(tokens)

    case parse_declaration(tokens) do
      {:ok, decl, rest} -> parse_declarations(rest, [decl | acc])
      {:error, _} when acc != [] -> {:ok, Enum.reverse(acc), tokens}
      other -> other
    end
  end
  </old_code>
  <description>
Recursively parses a sequence of top-level declarations (like functions, data types, imports).
It skips leading newlines, then attempts to parse a single declaration.
If successful, it adds the declaration to the accumulator and continues with the rest of the tokens.
If parsing a declaration fails but some declarations have already been accumulated, it returns the successfully parsed declarations and the remaining tokens (allowing for partial success or end of parsable declarations).
If parsing fails and no declarations were accumulated, it propagates the error.
  </description>
</function>

<function>
  <name>parse_declaration/1</name>
  <old_code>
def parse_declaration(tokens) do
    # try function+signature combo first
    case parse_function_with_type_signature(tokens) do
      {:ok, v, rest} ->
        {:ok, v, rest}

      {:error, _} ->
        case parse_type_signature(tokens) do
          {:ok, ts, rest} ->
            {:ok, ts, rest}

          {:error, _} ->
            parse_any(
              [
                &parse_module/1,
                &parse_import/1,
                &parse_foreign_import_simple/1,
                &parse_foreign_import/1,
                &parse_data_declaration/1,
                &parse_type_alias/1,
                &parse_type_class/1,
                &parse_type_class_instance/1,
                &parse_function_declaration/1
              ],
              tokens
            )
        end
    end
  end
  </old_code>
  <description>
Attempts to parse a single top-level declaration.
It prioritizes parsing a function with its type signature. If that fails, it tries to parse a standalone type signature.
If both fail, it uses `parse_any` to try a list of other declaration parsers: module, import (simple and full foreign), data, type alias, type class, type class instance, and standalone function declaration.
  </description>
</function>

<function>
  <name>parse_module/1</name>
  <old_code>
def parse_module(tokens) do
    tokens = drop_newlines(tokens)

    with {:ok, _, tokens} <- expect_keyword(tokens, "module"),
         {:ok, module_name, tokens} <- parse_qualified_identifier(tokens),
         {:ok, _, tokens} <- expect_keyword(tokens, "where") do
      {:ok, %Ast.Module{name: module_name}, tokens}
    else
      other -> other
    end
  end
  </old_code>
  <description>
Parses a module declaration, e.g., `module My.Module where`.
It expects the "module" keyword, followed by a qualified identifier for the module name, and then the "where" keyword.
Returns an `Ast.Module` node.
  </description>
</function>

<function>
  <name>parse_import/1</name>
  <old_code>
defp parse_import(tokens) do
    tokens = drop_newlines(tokens)

    with {:ok, _, tokens} <- expect_keyword(tokens, "import"),
         {:ok, mod, tokens} <- parse_qualified_identifier(tokens),
         {alias, tokens} <- parse_import_alias(tokens),
         {:ok, items, hiding?, tokens} <- parse_import_selectors(tokens) do
      {:ok,
       %Ast.ImportDeclaration{
         module: mod,
         alias: alias,
         items: items,
         hiding?: hiding?
       }, drop_newlines(tokens)}
    end
  end
  </old_code>
  <description>
Parses an import declaration, e.g., `import My.Module`, `import My.Module as Alias`, `import My.Module (item1, item2)`, or `import My.Module hiding (item3)`.
It expects the "import" keyword, a qualified module identifier, an optional alias (`as <Ident>`), and optional import selectors (specific items to import or hide).
Returns an `Ast.ImportDeclaration` node.
  </description>
</function>

<function>
  <name>parse_import_alias/1 (first clause)</name>
  <old_code>
defp parse_import_alias([%Token{type: :identifier, value: "as"} | rest]) do
    with {:ok, id, rest} <- parse_identifier(rest) do
      {id.name, rest}
    end
  end
  </old_code>
  <description>
Parses the alias part of an import statement, e.g., `as MyAlias`.
It expects an identifier "as" followed by another identifier for the alias name.
Returns a tuple `{alias_name_string, remaining_tokens}`.
  </description>
</function>

<function>
  <name>parse_import_alias/1 (second clause)</name>
  <old_code>
defp parse_import_alias(tokens), do: {nil, tokens}
  </old_code>
  <description>
Handles the case where there is no alias in an import statement.
Returns `{nil, original_tokens}`.
  </description>
</function>

<function>
  <name>parse_import_selectors/1 (hiding clause)</name>
  <old_code>
defp parse_import_selectors([%Token{type: :identifier, value: "hiding"} | rest]) do
    with {:ok, items, rest} <- parse_paren_import_list(rest) do
      {:ok, items, true, rest}
    end
  end
  </old_code>
  <description>
Parses the import selectors when the "hiding" keyword is present, e.g., `hiding (item1, item2)`.
It expects the "hiding" keyword followed by a parenthesized list of import items.
Returns `{:ok, items_list, true, remaining_tokens}` where `true` indicates it's a hiding list.
  </description>
</function>

<function>
  <name>parse_import_selectors/1 (selective import or no selectors)</name>
  <old_code>
defp parse_import_selectors(tokens) do
    case parse_paren_import_list(tokens) do
      {:ok, items, rest} -> {:ok, items, false, rest}
      {:error, _} -> {:ok, [], false, tokens}
    end
  end
  </old_code>
  <description>
Parses import selectors when "hiding" is not present.
It attempts to parse a parenthesized list of import items (selective import).
If successful, returns `{:ok, items_list, false, remaining_tokens}`.
If parsing a parenthesized list fails, it assumes no selectors were specified (importing everything by default, or as per language rules) and returns `{:ok, [], false, original_tokens}`.
  </description>
</function>

<function>
  <name>parse_paren_import_list/1 (first clause)</name>
  <old_code>
defp parse_paren_import_list([%Token{type: :delimiter, value: "("} | rest]) do
    with {:ok, items, rest} <-
           parse_separated(&parse_import_item/1, &expect_delimiter(&1, ","), rest),
         {:ok, _, rest} <- expect_delimiter(rest, ")") do
      {:ok, items, rest}
    end
  end
  </old_code>
  <description>
Parses a parenthesized, comma-separated list of import items, e.g., `(item1, Type(..), Op(Cons))`.
It expects an opening parenthesis, then calls `parse_separated` to parse items separated by commas, and finally expects a closing parenthesis.
  </description>
</function>

<function>
  <name>parse_paren_import_list/1 (second clause)</name>
  <old_code>
defp parse_paren_import_list(_), do: {:error, "no paren import list"}
  </old_code>
  <description>
Handles the case where the input tokens do not start with an opening parenthesis, indicating it's not a parenthesized import list.
  </description>
</function>

<function>
  <name>parse_import_item/1</name>
  <old_code>
defp parse_import_item(tokens) do
    with {:ok, first, tokens} <- parse_identifier(tokens) do
      case tokens do
        [%Token{type: :delimiter, value: "("} | _] ->
          # keep the "(" so parse_constructors/2 matches its head clause
          parse_constructors(tokens, first.name)

        _ ->
          {:ok, first.name, tokens}
      end
    end
  end
  </old_code>
  <description>
Parses a single item within an import list.
An item can be a simple identifier (e.g., `foo`), or an identifier followed by a parenthesized list of constructors/sub-items (e.g., `MyType(A, B)` or `MyType(..)`).
If it's a simple identifier, it returns the identifier's name.
If it's followed by `(`, it delegates to `parse_constructors/2`.
  </description>
</function>

<function>
  <name>parse_constructors/2</name>
  <old_code>
defp parse_constructors([%Token{type: :delimiter, value: "("} | rest], mod) do
    case rest do
      [%Token{type: :operator, value: "."}, %Token{type: :operator, value: "."} | rest2] ->
        {:ok, {mod, :all}, expect_delimiter(rest2, ")") |> elem(2)}

      [%Token{type: :operator, value: ".."} | rest2] ->
        {:ok, {mod, :all}, expect_delimiter(rest2, ")") |> elem(2)}

      _ ->
        with {:ok, ctors, rest2} <-
               parse_separated(&parse_identifier/1, &expect_delimiter(&1, ","), rest),
             {:ok, _, rest3} <- expect_delimiter(rest2, ")") do
          {:ok, {mod, Enum.map(ctors, & &1.name)}, rest3}
        end
    end
  end
  </old_code>
  <description>
Parses the list of constructors or sub-items for a type or module being imported, e.g., `(A, B)` or `(..)`.
It expects an opening parenthesis.
If `..` (either as two `.` tokens or one `..` token) is found, it signifies importing all constructors/sub-items, represented as `{module_name, :all}`.
Otherwise, it parses a comma-separated list of identifiers and returns them as `{module_name, [constructor_names_list]}`.
A closing parenthesis is expected at the end.
  </description>
</function>

<function>
  <name>parse_foreign_import_simple/1</name>
  <old_code>
defp parse_foreign_import_simple(tokens) do
    with {:ok, _, tokens} <- expect_keyword(tokens, "foreign"),
         {:ok, _, tokens} <- expect_keyword(tokens, "import"),
         {:ok, name, tokens} <- parse_identifier(tokens),
         {:ok, _, tokens} <- expect_operator(tokens, "::"),
         {:ok, type, tokens} <- parse_type(tokens) do
      {:ok,
       %Ast.ForeignImport{
         # not specified
         module: nil,
         # not specified
         function: nil,
         # the identifier itself
         alias: name.name,
         type_signature: %Ast.TypeSignature{
           name: name.name,
           type_vars: [],
           constraints: [],
           type: type
         }
       }, drop_newlines(tokens)}
    else
      _ -> {:error, "plain foreign import parse failed"}
    end
  end
  </old_code>
  <description>
Parses a simple foreign import declaration, e.g., `foreign import myFunction :: Int -> Int`.
This form doesn't specify the foreign module or function name explicitly, assuming conventions.
It expects "foreign import", an identifier for the alias, "::", and a type.
Returns an `Ast.ForeignImport` node with `module` and `function` fields set to `nil`.
  </description>
</function>

<function>
  <name>parse_foreign_import/1</name>
  <old_code>
defp parse_foreign_import(tokens) do
    tokens = drop_newlines(tokens)

    with {:ok, _, tokens} <- expect_keyword(tokens, "foreign"),
         {:ok, _, tokens} <- expect_keyword(tokens, "import"),
         # ⬇︎ target language is an identifier (keep it if you ever need it)
         {:ok, _lang, tokens} <- parse_identifier(tokens),
         {:ok, mod, tokens} <- parse_string_literal(tokens),
         {:ok, fun, tokens} <- parse_string_literal(tokens),
         {:ok, al, tokens} <- parse_identifier(tokens),
         {:ok, _, tokens} <- expect_operator(tokens, "::"),
         {:ok, type, tokens} <- parse_type(tokens) do
      {:ok,
       %Ast.ForeignImport{
         module: mod,
         function: fun,
         alias: al.name,
         type_signature: %Ast.TypeSignature{
           name: al.name,
           type_vars: [],
           constraints: [],
           type: type
         }
       }, tokens}
    else
      other -> other
    end
  end
  </old_code>
  <description>
Parses a detailed foreign import declaration, e.g., `foreign import elixir "Elixir.MyModule" "my_elixir_function" myFunctionAlias :: Args -> Result`.
It expects "foreign import", a target language identifier (currently parsed but unused), two string literals for the foreign module and function names, an alias identifier, "::", and a type.
Returns an `Ast.ForeignImport` node.
  </description>
</function>

<function>
  <name>parse_record_pattern/1 (first clause)</name>
  <old_code>
defp parse_record_pattern([%Token{type: :delimiter, value: "{"} | rest]) do
    with {:ok, fields, rest} <-
           parse_separated(&parse_record_field_pattern/1, &expect_delimiter(&1, ","), rest),
         {:ok, _, rest} <- expect_delimiter(rest, "}") do
      {:ok, %Ast.RecordPattern{fields: fields}, rest}
    end
  end
  </old_code>
  <description>
Parses a record pattern, e.g., `{ field1 = pattern1, field2 }`.
It expects an opening brace `{`, a comma-separated list of record field patterns parsed by `parse_record_field_pattern/1`, and a closing brace `}`.
Returns an `Ast.RecordPattern` node.
  </description>
</function>

<function>
  <name>parse_record_pattern/1 (second clause)</name>
  <old_code>
defp parse_record_pattern(_), do: {:error, "Expected record pattern"}
  </old_code>
  <description>
Handles the case where the input tokens do not start with an opening brace, indicating it's not a record pattern.
  </description>
</function>

<function>
  <name>parse_record_field_pattern/1</name>
  <old_code>
defp parse_record_field_pattern(tokens) do
    with {:ok, lbl, tokens} <- IO.inspect(parse_label(tokens)) do
      cond do
        # ──────────────── `:` delimiter ────────────────
        match?({:ok, _, _}, expect_delimiter(tokens, ":")) ->
          {:ok, _, tokens} = expect_delimiter(tokens, ":")

          with {:ok, pat, tokens} <- parse_pattern(tokens) do
            {:ok, {lbl.name, pat}, tokens}
          end

        # ──────────────── `=` operator ────────────────
        match?({:ok, _, _}, expect_operator(tokens, "=")) ->
          {:ok, _, tokens} = expect_operator(tokens, "=")

          with {:ok, pat, tokens} <- parse_pattern(tokens) do
            {:ok, {lbl.name, pat}, tokens}
          end

        # ─────────── shorthand  { head } ───────────────
        true ->
          {:ok, {lbl.name, %Ast.Identifier{name: lbl.name}}, tokens}
      end
    end
  end
  </old_code>
  <description>
Parses a single field within a record pattern.
It expects a label (field name).
Then, it can be followed by:
1.  `:` and a pattern (e.g., `label: subPattern`).
2.  `=` and a pattern (e.g., `label = subPattern`).
3.  Nothing (shorthand, e.g., `label`), which desugars to `label = Ast.Identifier{name: label}`.
Returns a tuple `{field_name_string, pattern_ast}`.
The `IO.inspect` call is for debugging and should ideally be removed in production code.
  </description>
</function>

<function>
  <name>parse_data_declaration/1</name>
  <old_code>
defp parse_data_declaration(tokens) do
    with {:ok, _, tokens} <- expect_keyword(tokens, "data"),
         {:ok, type_name, tokens} <- parse_identifier(tokens),
         {:ok, type_vars, tokens} <- parse_many(&parse_identifier/1, tokens),
         # <ΓöÇΓöÇ here
         tokens = skip_newlines(tokens),
         {:ok, _, tokens} <- expect_operator(tokens, "="),
         # <ΓöÇΓöÇ and here
         tokens = skip_newlines(tokens),
         {:ok, constructors, tokens} <- parse_data_constructors(tokens) do
      {:ok,
       %Ast.DataType{
         name: type_name.name,
         type_vars: Enum.map(type_vars, fn var -> var.name end),
         constructors: constructors
       }, tokens}
    else
      {:error, reason} -> {:error, reason}
    end
  end
  </old_code>
  <description>
Parses a data type declaration, e.g., `data Maybe a = Nothing | Just a`.
It expects the "data" keyword, the type name identifier, a list of type variable identifiers, an equals sign `=`, and then a list of data constructors.
Newlines are skipped around the equals sign.
Returns an `Ast.DataType` node.
  </description>
</function>

<function>
  <name>parse_data_constructors/1</name>
  <old_code>
defp parse_data_constructors(tokens) do
    parse_separated(&parse_data_constructor/1, &expect_operator(&1, "|"), tokens)
  end
  </old_code>
  <description>
Parses a list of data constructors, separated by the `|` operator, e.g., `Nothing | Just a | Another Int String`.
It uses `parse_separated` with `parse_data_constructor/1` as the item parser and `|` as the separator.
  </description>
</function>

<function>
  <name>parse_data_constructor/1</name>
  <old_code>
defp parse_data_constructor(tokens) do
    with {:ok, constructor_name, tokens} <- parse_identifier(tokens),
         # ⬇️  collect atomic types, not full applications
         {:ok, fields, tokens} <- parse_many(&parse_type_atom/1, tokens) do
      {:ok,
       %Ast.DataConstructor{
         name: constructor_name.name,
         fields: fields
       }, tokens}
    else
      {:error, reason} -> {:error, reason}
    end
  end
  </old_code>
  <description>
Parses a single data constructor, e.g., `Just a` or `Person String Int`.
It expects the constructor name (an identifier) followed by zero or more type atoms representing its fields/arguments.
Returns an `Ast.DataConstructor` node.
  </description>
</function>

<function>
  <name>skip_superclass_constraints/1</name>
  <old_code>
defp skip_superclass_constraints(tokens) do
    tokens = skip_newlines(tokens)

    {before, after_} =
      Enum.split_while(tokens, fn
        %Token{type: :operator, value: "<="} -> false
        _ -> true
      end)

    case after_ do
      [%Token{type: :operator, value: "<="} | rest] -> {rest, before}
      _ -> {tokens, []}
    end
  end
  </old_code>
  <description>
Skips superclass constraints in a type class declaration.
It splits the token list at the first occurrence of the `<=` operator.
Returns a tuple `{remaining_tokens_after_<=, constraint_tokens_before_<=}`.
If `<=` is not found, it returns `{original_tokens, []}`.
This is used to separate constraints like `(Applicative m, Bind m)` from the rest of `class ... <= Monad m where ...`.
The `constraint_tokens_before_<=` are currently ignored by the caller (`parse_type_class`).
  </description>
</function>

<function>
  <name>drop_instance_constraints/1</name>
  <old_code>
defp drop_instance_constraints(tokens) do
    {_before, after_} =
      Enum.split_while(tokens, fn
        %Token{type: :operator, value: "<="} -> false
        _ -> true
      end)

    case after_ do
      [%Token{type: :operator, value: "<="} | rest] -> rest
      _ -> tokens
    end
  end
  </old_code>
  <description>
Drops leading instance constraints in a type class instance declaration.
Similar to `skip_superclass_constraints/1`, it looks for the `<=` operator.
If found, it returns the tokens *after* the `<=`. If not found, it returns the original tokens.
The constraint tokens themselves are discarded. Used for syntax like `instance (Eq a) <= Show a where ...`.
  </description>
</function>

<function>
  <name>parse_type_class/1</name>
  <old_code>
defp parse_type_class(tokens) do
    with {:ok, _, tokens} <- expect_keyword(tokens, "class") do
      {tokens, _constraints} = skip_superclass_constraints(tokens)

      with {:ok, class_name, tokens} <- parse_identifier(tokens),
           {:ok, type_vars, tokens} <- parse_many(&parse_identifier/1, tokens),
           {:ok, _, tokens} <- expect_keyword(tokens, "where"),
           {:ok, methods, tokens} <- parse_many(&parse_type_signature/1, tokens) do
        {:ok,
         %Ast.TypeClass{
           name: class_name.name,
           type_vars: Enum.map(type_vars, & &1.name),
           methods: methods
         }, tokens}
      end
    end
  end
  </old_code>
  <description>
Parses a type class declaration, e.g., `class Functor f where map :: (a -> b) -> f a -> f b` or `class (Applicative m, Bind m) <= Monad m where ...`.
It expects the "class" keyword. It then skips optional superclass constraints (using `skip_superclass_constraints/1`).
After constraints, it parses the class name identifier, a list of type variable identifiers, the "where" keyword, and a list of method type signatures.
Returns an `Ast.TypeClass` node. The parsed constraints are currently ignored.
  </description>
</function>

<function>
  <name>parse_type_class_instance/1</name>
  <old_code>
defp parse_type_class_instance(tokens) do
    with {:ok, _, tokens} <- expect_keyword(tokens, "instance") do
      tokens = drop_newlines(tokens)

      case tokens do
        # ── Named instance – starts with ident followed by '::' ──
        [%Token{type: :identifier, value: inst_name}, %Token{type: :operator, value: "::"} | rest] ->
          rest = drop_instance_constraints(rest)

          with {:ok, type_ast, tokens} <- parse_type(rest),
               {:ok, _, tokens} <- expect_keyword(tokens, "where"),
               {:ok, methods, tokens} <- parse_many(&parse_function_declaration/1, tokens) do
            class_name =
              case type_ast do
                %Ast.FunctionCall{function: %Ast.Identifier{name: cn}} -> cn
                %Ast.Identifier{name: cn} -> cn
                _ -> inst_name
              end

            {:ok,
             %Ast.TypeClassInstance{
               class_name: class_name,
               type: type_ast,
               methods: methods
             }, tokens}
          end

        # ── Unnamed instance – old syntax: Class Type where … ──
        _ ->
          tokens = drop_instance_constraints(tokens)

          with {:ok, class_name, tokens} <- parse_identifier(tokens),
               {:ok, type_ast, tokens} <- parse_type(tokens),
               {:ok, _, tokens} <- expect_keyword(tokens, "where"),
               {:ok, methods, tokens} <- parse_many(&parse_function_declaration/1, tokens) do
            {:ok,
             %Ast.TypeClassInstance{
               class_name: class_name.name,
               type: type_ast,
               methods: methods
             }, tokens}
          end
      end
    end
  end
  </old_code>
  <description>
Parses a type class instance declaration. Supports two main forms:
1.  Named instance: `instance instanceName :: OptionalConstraints <= ClassName TypeVars where ...` (e.g., `instance showString :: Show String where ...`). It looks for an identifier followed by `::`.
2.  Unnamed instance: `instance OptionalConstraints <= ClassName TypeVars where ...` (e.g., `instance (Eq a) <= Show a where ...`). This is the fallback if the named pattern doesn't match.

In both cases, it first expects the "instance" keyword, then drops optional instance constraints using `drop_instance_constraints/1`.
For named instances, it parses the instance name, `::`, the type, "where", and method implementations. The class name is inferred from the type AST or defaults to the instance name.
For unnamed instances, it parses the class name identifier, the type, "where", and method implementations.
Returns an `Ast.TypeClassInstance` node.
  </description>
</function>

<function>
  <name>split_type_and_rest/2</name>
  <old_code>
defp split_type_and_rest(tokens, name) do
    Enum.split_while(tokens, fn
      %Token{type: :identifier, value: ^name} -> false
      _ -> true
    end)
  end
  </old_code>
  <description>
Splits a list of tokens into two parts: tokens belonging to a type signature and the rest of the tokens.
The split occurs just before an identifier token that matches the given `name`. This `name` is expected to be the start of the function definition that follows the type signature.
This is used in `parse_function_with_type_signature` to separate the type part from the function body part when they are declared together.
For example, in `foo :: Int -> Int\nfoo x = x`, given `name = "foo"`, it would split after `Int -> Int` and before the second `foo`.
  </description>
</function>

<function>
  <name>parse_function_with_type_signature/1</name>
  <old_code>
defp parse_function_with_type_signature(tokens) do
    tokens = drop_newlines(tokens)

    case tokens do
      [%Token{type: :identifier, value: name} | rest1] ->
        with {:ok, _, rest2} <- expect_operator(rest1, "::"),
             {type_tokens, rest3} <- split_type_and_rest(rest2, name),
             {:ok, type_ast, []} <- parse_type(strip_newlines(type_tokens)),
             {:ok, fun_ast, final} <- parse_function_declaration(rest3),
             true <- fun_ast.name == name do
          {:ok,
           %Ast.FunctionDeclaration{
             name: fun_ast.name,
             parameters: fun_ast.parameters,
             body: fun_ast.body,
             type_signature: %Ast.TypeSignature{
               name: name,
               type_vars: [],
               constraints: [],
               type: type_ast
             }
           }, final}
        else
          _ -> {:error, "function-with-signature parse failed"}
        end

      _ ->
        {:error, "Expected identifier at start of type signature"}
    end
  end
  </old_code>
  <description>
Parses a function declaration that is immediately preceded by its type signature, e.g.,
`myFunc :: Int -> String`
`myFunc x = "hello"`
It first expects an identifier (function name), then `::`.
It uses `split_type_and_rest/2` to separate the tokens for the type from the tokens for the function definition.
It then parses the type and the function definition independently.
If the parsed function's name matches the name from the signature, it combines them into a single `Ast.FunctionDeclaration` node with the `type_signature` field populated.
  </description>
</function>

<function>
  <name>parse_type_signature/1</name>
  <old_code>
defp parse_type_signature(tokens) do
    tokens = drop_newlines(tokens)

    with {:ok, name, tokens} <- parse_identifier(tokens),
         {:ok, _, tokens} <- expect_operator(tokens, "::"),
         {:ok, type, tokens} <- parse_type(tokens) do
      {:ok, %Ast.TypeSignature{name: name.name, type_vars: [], constraints: [], type: type},
       tokens}
    else
      other -> other
    end
  end
  </old_code>
  <description>
Parses a standalone type signature, e.g., `myFunction :: Int -> String`.
It expects an identifier (the name of the function/value being typed), the `::` operator, and then a type (parsed by `parse_type/1`).
Returns an `Ast.TypeSignature` node. `type_vars` and `constraints` are currently initialized as empty.
  </description>
</function>

<function>
  <name>parse_type/1 (forall clause)</name>
  <old_code>
defp parse_type([%Token{type: :identifier, value: "forall"} | _] = toks),
    do: parse_forall_type(toks)
  </old_code>
  <description>
Entry point for type parsing. If the tokens start with the identifier "forall", it delegates to `parse_forall_type/1` to handle polymorphic types.
  </description>
</function>

<function>
  <name>parse_type/1 (function type clause)</name>
  <old_code>
defp parse_type(toks), do: parse_function_type(toks)
  </old_code>
  <description>
Entry point for type parsing. If the type does not start with "forall", it delegates to `parse_function_type/1`, which handles function types and other type terms.
  </description>
</function>

<function>
  <name>parse_forall_type/1</name>
  <old_code>
defp parse_forall_type([%Token{value: "forall"} | rest]) do
    with {:ok, vars, rest} <- parse_many(&parse_identifier/1, rest),
         {:ok, _, rest} <- expect_operator(rest, "."),
         {:ok, ty, rest} <- parse_type(rest) do
      {:ok, %Ast.ForAllType{vars: Enum.map(vars, & &1.name), type: ty}, rest}
    end
  end
  </old_code>
  <description>
Parses a universally quantified type (forall type), e.g., `forall a b. a -> b -> a`.
It expects the "forall" keyword, followed by one or more type variable identifiers, a dot `.`, and then the body of the type (parsed recursively by `parse_type/1`).
Returns an `Ast.ForAllType` node.
  </description>
</function>

<function>
  <name>parse_type_alias/1</name>
  <old_code>
defp parse_type_alias(tokens) do
    with {:ok, _, tokens} <- expect_keyword(tokens, "type"),
         {:ok, name, tokens} <- parse_identifier(tokens),
         {:ok, vars, tokens} <- parse_many(&parse_identifier/1, tokens),
         {:ok, _, tokens} <- expect_operator(tokens, "="),
         tokens = skip_newlines(tokens),
         {:ok, aliased, tokens} <- parse_type(tokens) do
      {:ok,
       %Ast.TypeAlias{
         name: name.name,
         type_vars: Enum.map(vars, & &1.name),
         type: aliased
       }, tokens}
    else
      other -> other
    end
  end
  </old_code>
  <description>
Parses a type alias declaration, e.g., `type StringList = List String` or `type Pair a b = (a, b)`.
It expects the "type" keyword, the alias name identifier, a list of type variable identifiers, an equals sign `=`, and then the type being aliased (parsed by `parse_type/1`).
Returns an `Ast.TypeAlias` node.
  </description>
</function>

<function>
  <name>parse_function_type/1</name>
  <old_code>
defp parse_function_type(tokens) do
    with {:ok, left, tokens} <- parse_type_term(tokens) do
      case tokens do
        [%Token{type: :operator, value: "->"} | rest] ->
          with {:ok, right, rest} <- parse_function_type(rest) do
            {:ok, %Ast.BinaryOp{op: "->", left: left, right: right}, rest}
          end

        _ ->
          {:ok, left, tokens}
      end
    end
  end
  </old_code>
  <description>
Parses a function type, which is right-associative, e.g., `A -> B -> C` is parsed as `A -> (B -> C)`.
It first parses a `type_term` (the left-hand side of a potential arrow).
If an arrow `->` operator follows, it recursively calls `parse_function_type` for the right-hand side and constructs an `Ast.BinaryOp` node for the function type.
If no arrow follows, it means the parsed `type_term` is the complete type.
  </description>
</function>

<function>
  <name>parse_record_type/1 (first clause)</name>
  <old_code>
def parse_record_type([%Token{type: :delimiter, value: "{"} | rest]) do
    with {:ok, fields, rest} <-
           parse_separated(&parse_record_field/1, &expect_delimiter(&1, ","), rest),
         {:ok, _, rest} <- expect_delimiter(rest, "}") do
      {:ok, %Ast.RecordType{fields: fields}, rest}
    end
  end
  </old_code>
  <description>
Parses a record type definition, e.g., `{ name :: String, age :: Int }`.
It expects an opening brace `{`, a comma-separated list of record fields (parsed by `parse_record_field/1`), and a closing brace `}`.
Returns an `Ast.RecordType` node.
  </description>
</function>

<function>
  <name>parse_record_type/1 (second clause)</name>
  <old_code>
def parse_record_type(_), do: {:error, "Expected record type"}
  </old_code>
  <description>
Handles the case where the input tokens do not start with an opening brace, indicating it's not a record type.
  </description>
</function>

<function>
  <name>parse_record_field/1</name>
  <old_code>
defp parse_record_field(tokens) do
    with {:ok, label, tokens} <- parse_label(tokens),
         {:ok, _, tokens} <- expect_operator(tokens, "::"),
         tokens = skip_newlines(tokens),
         {:ok, t, tokens} <- parse_type(tokens) do
      {:ok, {label.name, t}, tokens}
    end
  end
  </old_code>
  <description>
Parses a single field within a record type definition, e.g., `fieldName :: FieldType`.
It expects a label (field name), the `::` operator, and then a type (parsed by `parse_type/1`).
Returns a tuple `{field_name_string, type_ast}`.
  </description>
</function>

<function>
  <name>parse_type_term/1</name>
  <old_code>
defp parse_type_term(tokens) do
    parse_any(
      [
        &parse_record_type/1,
        &parse_list_type/1,
        &parse_tuple_type/1,
        &parse_basic_type/1
      ],
      tokens
    )
  end
  </old_code>
  <description>
Parses a "type term", which is a non-function type component. This can be a record type, list type, tuple type, or a basic type (identifier, qualified identifier, or type application).
It uses `parse_any` to try parsing these in order. This forms the operands for function types (`->`).
  </description>
</function>

<function>
  <name>parse_list_type/1</name>
  <old_code>
defp parse_list_type(tokens) do
    case tokens do
      [%Token{type: :delimiter, value: "["} | rest] ->
        with {:ok, element_type, rest} <- parse_type(rest),
             {:ok, _, rest} <- expect_delimiter(rest, "]") do
          {:ok, %Ast.FunctionCall{function: "[]", arguments: [element_type]}, rest}
        end

      _ ->
        {:error, "Expected list type"}
    end
  end
  </old_code>
  <description>
Parses a list type, e.g., `[Int]`.
It expects an opening square bracket `[`, a type for the list elements (parsed by `parse_type/1`), and a closing square bracket `]`.
Returns an `Ast.FunctionCall` node representing the list type constructor, typically with `function: "[]"` and the element type as an argument.
  </description>
</function>

<function>
  <name>parse_tuple_type/1</name>
  <old_code>
defp parse_tuple_type(tokens) do
    case tokens do
      [%Token{type: :delimiter, value: "("} | rest] ->
        with {:ok, elements, rest} <-
               parse_separated(&parse_type/1, &expect_delimiter(&1, ","), rest),
             {:ok, _, rest} <- expect_delimiter(rest, ")") do
          if length(elements) == 1 do
            # Parentheses used only for grouping → return the inner type
            {:ok, List.first(elements), rest}
          else
            # Real tuple type
            {:ok, %Ast.Tuple{elements: elements}, rest}
          end
        end

      _ ->
        {:error, "Expected tuple type"}
    end
  end
  </old_code>
  <description>
Parses a tuple type, e.g., `(Int, String)` or `(Bool)`.
It expects an opening parenthesis `(`, a comma-separated list of types (parsed by `parse_type/1`), and a closing parenthesis `)`.
If there's only one element inside the parentheses, it's treated as a grouped type, and the inner type is returned directly.
Otherwise, it returns an `Ast.Tuple` node containing the element types.
  </description>
</function>

<function>
  <name>parse_basic_type/1</name>
  <old_code>
defp parse_basic_type(tokens) do
    case parse_qualified_identifier(tokens) do
      {:ok, qid, rest} ->
        # Gather any type arguments that follow the qualified name
        case parse_many(&parse_type_atom/1, rest) do
          {:ok, [], ^rest} ->
            {:ok, qid, rest}

          {:ok, args, new_rest} ->
            {:ok, %Ast.FunctionCall{function: qid, arguments: args}, new_rest}
        end

      # fall back to the old rules
      _ ->
        parse_basic_type_fallback(tokens)
    end
  end
  </old_code>
  <description>
Parses a basic type, which can be a qualified identifier (e.g., `Maybe.Maybe a`) or a simple identifier, possibly followed by type arguments (type application).
It first attempts to parse a qualified identifier. If successful, it then tries to parse following type arguments using `parse_many(&parse_type_atom/1, ...)`.
If arguments are found, it constructs an `Ast.FunctionCall` (representing type application). If no arguments, it's just the qualified identifier.
If parsing a qualified identifier fails, it falls back to `parse_basic_type_fallback/1`.
  </description>
</function>

<function>
  <name>parse_basic_type_fallback/1</name>
  <old_code>
defp parse_basic_type_fallback(tokens) do
    case tokens do
      [%Token{type: :identifier, value: name} | rest] ->
        # Parse type arguments if any
        case parse_many(&parse_type_atom/1, rest) do
          {:ok, [], ^rest} ->
            # No arguments, just an identifier
            {:ok, %Ast.Identifier{name: name}, rest}

          {:ok, args, new_rest} ->
            # Type with arguments
            {:ok, %Ast.FunctionCall{function: name, arguments: args}, new_rest}
        end

      [%Token{type: :delimiter, value: "("} | rest] ->
        with {:ok, type, rest} <- parse_type(rest),
             {:ok, _, rest} <- expect_delimiter(rest, ")") do
          {:ok, type, rest}
        end

      _ ->
        {:error, "Expected basic type"}
    end
  end
  </old_code>
  <description>
Fallback parser for basic types, used if `parse_basic_type/1` fails to parse a qualified identifier at the start.
It handles:
1.  A simple identifier, possibly followed by type arguments (parsed by `parse_many(&parse_type_atom/1, ...)`). If arguments exist, it's an `Ast.FunctionCall` (type application); otherwise, an `Ast.Identifier`.
2.  A parenthesized type (e.g., `(Int -> String)`), which parses the inner type and returns it (handling grouping).
  </description>
</function>

<function>
  <name>parse_type_atom/1</name>
  <old_code>
def parse_type_atom(tokens) do
    case tokens do
      [%Token{type: :delimiter, value: "{"} | _] = toks ->
        parse_record_type(toks)

      [%Token{type: :identifier, value: name} | _] = toks ->
        parse_qualified_identifier(toks)

      [%Token{type: :delimiter, value: "("} | rest] ->
        with {:ok, type, rest} <- parse_type(rest),
             {:ok, _, rest} <- expect_delimiter(rest, ")") do
          {:ok, type, rest}
        end

      [%Token{type: :delimiter, value: "["} | rest] ->
        with {:ok, element_type, rest} <- parse_type(rest),
             {:ok, _, rest} <- expect_delimiter(rest, "]") do
          {:ok, %Ast.FunctionCall{function: "[]", arguments: [element_type]}, rest}
        end

      _ ->
        {:error, "Expected type atom"}
    end
  end
  </old_code>
  <description>
Parses a "type atom", which is the most basic, non-decomposable unit of a type when considering type application arguments.
It can be:
1.  A record type (e.g., `{ x :: Int }`).
2.  A qualified identifier (e.g., `MyModule.MyType`). This is tried if it starts with an identifier.
3.  A parenthesized type (e.g., `(Int -> String)`), which effectively parses the inner type.
4.  A list type (e.g., `[Int]`).
This is used to parse arguments to type constructors in type applications like `Map Key Value`.
  </description>
</function>

<function>
  <name>parse_function_declaration/1</name>
  <old_code>
defp parse_function_declaration(tokens) do
    with {:ok, name, tokens} <- parse_identifier(tokens),
         {:ok, parameters, tokens} <- parse_function_parameters(tokens),
         {:ok, _, tokens} <- expect_operator(tokens, "="),
         {:ok, body, tokens} <- parse_expression(tokens) do
      {:ok,
       %Ast.FunctionDeclaration{
         name: name.name,
         # ← keep full pattern nodes
         parameters: parameters,
         body: body,
         type_signature: nil
       }, tokens}
    else
      {:error, reason} -> {:error, reason}
    end
  end
  </old_code>
  <description>
Parses a function declaration without an explicit preceding type signature, e.g., `add x y = x + y`.
It expects an identifier for the function name, a list of parameters (parsed by `parse_function_parameters/1`), an equals sign `=`, and an expression for the function body.
Returns an `Ast.FunctionDeclaration` node with `type_signature` set to `nil`. The parameters are full pattern AST nodes.
  </description>
</function>

<function>
  <name>parse_function_parameters/1</name>
  <old_code>
defp parse_function_parameters(tokens), do: parse_many(&parse_simple_pattern/1, tokens)
  </old_code>
  <description>
Parses a sequence of zero or more function parameters. Each parameter is parsed as a "simple pattern" using `parse_simple_pattern/1`.
It uses `parse_many` to collect all such patterns.
  </description>
</function>

<function>
  <name>parse_simple_pattern/1</name>
  <old_code>
defp parse_simple_pattern(tokens) do
    parse_any(
      [
        &parse_literal/1,
        &parse_identifier/1,
        &parse_tuple_pattern/1,
        &parse_list_pattern/1,
        fn
          [%Token{type: :delimiter, value: "("} | rest] ->
            with {:ok, pattern, rest} <- parse_pattern(rest),
                 {:ok, _, rest} <- expect_delimiter(rest, ")") do
              {:ok, pattern, rest}
            end

          _ ->
            {:error, "Expected parenthesized pattern"}
        end
      ],
      tokens
    )
  end
  </old_code>
  <description>
Parses a "simple pattern," typically used for function parameters. These are patterns that don't involve complex constructions like record patterns or constructor patterns with arguments (though the latter might be covered by `parse_identifier` if it's a nullary constructor).
It tries to parse: a literal, an identifier, a tuple pattern, a list pattern, or a parenthesized pattern (which then uses the more general `parse_pattern/1`).
  </description>
</function>

<function>
  <name>parse_pattern/1</name>
  <old_code>
def parse_pattern(tokens) do
    parse_any(
      [
        &parse_record_pattern/1,
        &parse_wildcard_pattern/1,
        &parse_cons_pattern/1,
        &parse_constructor_pattern/1,
        &parse_tuple_pattern/1,
        &parse_list_pattern/1,
        &parse_literal/1,
        &parse_identifier/1,
        fn tokens ->
          case tokens do
            [%Token{type: :delimiter, value: "("} | rest] ->
              with {:ok, pattern, rest} <- parse_pattern(rest),
                   {:ok, _, rest} <- expect_delimiter(rest, ")") do
                {:ok, pattern, rest}
              end

            _ ->
              {:error, "Expected parenthesized pattern"}
          end
        end
      ],
      tokens
    )
  end
  </old_code>
  <description>
Parses a general pattern, used in contexts like `case` clauses or `let` bindings.
It uses `parse_any` to try parsing various pattern types in a specific order of precedence/specificity:
record pattern, wildcard (`_`), cons pattern (`h:t`), constructor pattern (e.g., `Just x`), tuple pattern, list pattern, literal, identifier, or a parenthesized pattern (which recursively calls `parse_pattern`).
  </description>
</function>

<function>
  <name>parse_constructor_pattern/1</name>
  <old_code>
defp parse_constructor_pattern(tokens) do
    with {:ok, constructor, tokens} <- parse_identifier(tokens),
         {:ok, args, tokens} <- parse_many(&parse_pattern/1, tokens) do
      if args == [] do
        {:ok, %Ast.Identifier{name: constructor.name}, tokens}
      else
        {:ok,
         %Ast.FunctionCall{
           # <- wrap
           function: %Ast.Identifier{name: constructor.name},
           arguments: args
         }, tokens}
      end
    else
      {:error, _} -> {:error, "Expected constructor pattern"}
    end
  end
  </old_code>
  <description>
Parses a data constructor pattern, e.g., `Nothing`, `Just x`, or `Node left right`.
It expects an identifier for the constructor name, followed by zero or more argument patterns (parsed by `parse_many(&parse_pattern/1, ...)`).
If there are no arguments, it's treated as a simple `Ast.Identifier` (representing a nullary constructor).
If there are arguments, it's represented as an `Ast.FunctionCall` where the `function` is the constructor identifier and `arguments` are the parsed sub-patterns.
  </description>
</function>

<function>
  <name>parse_cons_pattern/1</name>
  <old_code>
defp parse_cons_pattern(tokens) do
    with {:ok, head, tokens} <- parse_simple_pattern(tokens),
         # the ":" itself
         {:ok, _, tokens} <- expect_colon(tokens),
         {:ok, tail, tokens} <- parse_pattern(tokens) do
      {:ok,
       %Ast.FunctionCall{
         function: %Ast.Identifier{name: ":"},
         arguments: [head, tail]
       }, tokens}
    else
      _ -> {:error, "Expected cons pattern"}
    end
  end
  </old_code>
  <description>
Parses a list cons pattern, e.g., `head : tail`.
It expects a "simple pattern" for the head, followed by a colon `:` operator (checked by `expect_colon/1`), and then a general pattern for the tail.
Represents this as an `Ast.FunctionCall` with the function being an `Ast.Identifier{name: ":"}` and arguments being the head and tail patterns.
  </description>
</function>

<function>
  <name>expect_colon/1 (first clause)</name>
  <old_code>
defp expect_colon([%Token{value: ":"} | rest]), do: {:ok, ":", rest}
  </old_code>
  <description>
Expects and consumes a colon token. This specifically matches a token whose `value` is `":"`.
This is likely intended for the cons operator, which might be tokenized differently from a general delimiter or operator `::`.
  </description>
</function>

<function>
  <name>expect_colon/1 (second clause)</name>
  <old_code>
defp expect_colon(_), do: {:error, "Expected ':'"}
  </old_code>
  <description>
Handles the case where a colon token is expected but not found.
  </description>
</function>

<function>
  <name>parse_wildcard_pattern/1 (first clause)</name>
  <old_code>
def parse_wildcard_pattern([%Token{type: :identifier, value: "_"} | rest]),
    do: {:ok, %Ast.Wildcard{}, rest}
  </old_code>
  <description>
Parses a wildcard pattern, represented by the underscore `_`.
It expects an identifier token with the value `"_"` and returns an `Ast.Wildcard` node.
  </description>
</function>

<function>
  <name>parse_wildcard_pattern/1 (second clause)</name>
  <old_code>
def parse_wildcard_pattern(_), do: {:error, "Expected wildcard"}
  </old_code>
  <description>
Handles the case where a wildcard pattern is expected but the token is not an underscore identifier.
  </description>
</function>

<function>
  <name>parse_tuple_pattern/1</name>
  <old_code>
defp parse_tuple_pattern(tokens) do
    case tokens do
      [%Token{type: :delimiter, value: "("} | rest] ->
        with {:ok, elements, rest} <-
               parse_separated(&parse_pattern/1, &expect_delimiter(&1, ","), rest),
             {:ok, _, rest} <- expect_delimiter(rest, ")") do
          {:ok, %Ast.Tuple{elements: elements}, rest}
        end

      _ ->
        {:error, "Expected tuple pattern"}
    end
  end
  </old_code>
  <description>
Parses a tuple pattern, e.g., `(p1, p2, p3)` or `()`.
It expects an opening parenthesis `(`, a comma-separated list of patterns (parsed by `parse_pattern/1`), and a closing parenthesis `)`.
Unlike `parse_tuple_type` or `parse_tuple_literal`, it does not special-case single-element tuples; `(p1)` would be parsed as a tuple pattern with one element if the grammar allows (or as a grouped pattern by `parse_pattern`'s parenthesized rule). Here, it always forms an `Ast.Tuple`.
  </description>
</function>

<function>
  <name>parse_list_pattern/1</name>
  <old_code>
defp parse_list_pattern(tokens) do
    case tokens do
      [%Token{type: :delimiter, value: "["} | rest] ->
        with {:ok, elements, rest} <-
               parse_separated(&parse_pattern/1, &expect_delimiter(&1, ","), rest),
             {:ok, _, rest} <- expect_delimiter(rest, "]") do
          {:ok, %Ast.List{elements: elements}, rest}
        end

      [%Token{type: :delimiter, value: "["} | rest] -> # This clause seems redundant or incorrectly placed due to the one above.
        {:ok, %Ast.List{elements: []}, rest} # Likely intended for `[]` specifically, but the above `parse_separated` handles empty lists if `parser` can fail gracefully or `separator` is not found initially.

      _ ->
        {:error, "Expected list pattern"}
    end
  end
  </old_code>
  <description>
Parses a list pattern, e.g., `[p1, p2]`, `[]`, or `[p]`.
It expects an opening square bracket `[`.
The first clause attempts to parse a comma-separated list of patterns (parsed by `parse_pattern/1`) followed by a closing square bracket `]`. This correctly handles `[]` if `parse_separated` returns an empty list when no elements are found before the closing bracket.
The second clause `[%Token{type: :delimiter, value: "["} | rest] -> {:ok, %Ast.List{elements: []}, rest}` seems to be an attempt to handle the empty list `[]` specifically. However, if `parse_separated` is robust, this might be redundant or could lead to ambiguity if not ordered carefully or if the first clause doesn't consume the `]` correctly for an empty list. Assuming `parse_separated` can return `{:ok, [], rest_after_empty_list}`, the first clause should handle `[]` by parsing zero elements. If `parse_separated` requires at least one element, then a specific clause for `[]` would be needed, but it should probably look like `[%Token{type: :delimiter, value: "["}, %Token{type: :delimiter, value: "]"}|rest]`.
Given the current structure, if `parse_separated` successfully parses an empty list of elements (e.g., when `rest` immediately contains `"]"`), the first clause handles `[]`. If `parse_separated` fails on an empty list, the second clause would be hit, but it doesn't consume the closing `]`. This needs careful review of `parse_separated` behavior.

*Correction based on typical `parse_separated` behavior:* If `parse_separated` requires the first item to parse successfully, then `[]` would not be matched by the first `with` block. A specific clause for `[]` would be `[%Token{type: :delimiter, value: "["}, %Token{type: :delimiter, value: "]"}|rest] -> {:ok, %Ast.List{elements: []}, rest}`. The current second clause is problematic as it consumes `[` but not `]`.

Assuming the intent is that `parse_separated` handles the empty case correctly within the brackets:
It returns an `Ast.List` node.
  </description>
</function>

<function>
  <name>parse_lambda/1</name>
  <old_code>
defp parse_lambda(tokens) do
    with {:ok, _, tokens} <- expect_operator(tokens, "\\"),
         {:ok, parameters, tokens} <- parse_function_parameters(tokens),
         {:ok, _, tokens} <- expect_operator(tokens, "->"),
         {:ok, body, tokens} <- parse_expression(tokens) do
      {:ok, %Ast.Lambda{parameters: parameters, body: body}, tokens}
    else
      {:error, reason} -> {:error, reason}
    end
  end
  </old_code>
  <description>
Parses a lambda expression (anonymous function), e.g., `\x y -> x + y`.
It expects a backslash `\` operator, followed by a list of parameters (parsed by `parse_function_parameters/1`), an arrow `->` operator, and an expression for the lambda body.
Returns an `Ast.Lambda` node.
  </description>
</function>

<function>
  <name>parse_expression/1</name>
  <old_code>
def parse_expression(tokens) do
    parse_binary_expression(tokens)
  end
  </old_code>
  <description>
Main entry point for parsing an expression. It delegates to `parse_binary_expression/1`, which handles operator precedence and other expression forms.
  </description>
</function>

<function>
  <name>parse_let_expression/1</name>
  <old_code>
def parse_let_expression(tokens) do
    tokens = skip_newlines(tokens)

    with {:ok, _, tokens} <- expect_keyword(tokens, "let"),
         # gap A
         tokens = skip_newlines(tokens),
         {:ok, bindings, tokens} <- parse_many(&parse_binding/1, tokens),
         # gap B
         tokens = skip_newlines(tokens),
         {:ok, _, tokens} <- expect_keyword(tokens, "in"),
         # gap C
         tokens = skip_newlines(tokens),
         {:ok, body, tokens} <- parse_expression(tokens) do
      {:ok, %Ast.LetBinding{bindings: bindings, body: body}, tokens}
    else
      {:error, reason} -> {:error, reason}
    end
  end
  </old_code>
  <description>
Parses a let-expression, e.g., `let x = 1; y = 2 in x + y`.
It expects the "let" keyword, followed by one or more bindings (parsed by `parse_many(&parse_binding/1, ...)`), the "in" keyword, and then an expression for the body of the let-expression.
It is layout-aware, skipping newlines at specific points (after "let", before "in", after "in").
Bindings are expected to be separated by newlines or parsed sequentially by `parse_many` if `parse_binding` consumes its own trailing newlines or if they are on the same line separated by semicolons (if supported by tokenizer/`parse_binding`).
Returns an `Ast.LetBinding` node.
  </description>
</function>

<function>
  <name>parse_binding/1</name>
  <old_code>
def parse_binding(tokens) do
    tokens = skip_newlines(tokens)

    with {:ok, pat, tokens} <- parse_pattern(tokens),
         {:ok, _, tokens} <- expect_operator(tokens, "="),
         {:ok, rhs, tokens} <- parse_expression(tokens) do
      {:ok, {pat, rhs}, tokens}
    else
      {:error, reason} -> {:error, reason}
    end
  end
  </old_code>
  <description>
Parses a single binding within a let-expression, e.g., `pattern = expression`.
It skips leading newlines, then expects a pattern (parsed by `parse_pattern/1`), an equals sign `=`, and an expression for the right-hand side.
Returns a tuple `{pattern_ast, expression_ast}`.
  </description>
</function>

<function>
  <name>parse_if_expression/1</name>
  <old_code>
defp parse_if_expression(tokens) do
    tokens = skip_newlines(tokens)

    with {:ok, _, tokens} <- expect_keyword(tokens, "if"),
         {:ok, condition, tokens} <- parse_expression(tokens),
         {:ok, _, tokens} <- expect_keyword(tokens, "then"),
         {:ok, then_branch, tokens} <- parse_expression(tokens),
         {:ok, _, tokens} <- expect_keyword(tokens, "else"),
         {:ok, else_branch, tokens} <- parse_expression(tokens) do
      {:ok,
       %Ast.IfExpression{
         condition: condition,
         then_branch: then_branch,
         else_branch: else_branch
       }, tokens}
    else
      {:error, reason} -> {:error, reason}
    end
  end
  </old_code>
  <description>
Parses an if-then-else expression, e.g., `if condition then expr1 else expr2`.
It expects the "if" keyword, an expression for the condition, the "then" keyword, an expression for the then-branch, the "else" keyword, and an expression for the else-branch.
Skips leading newlines before "if".
Returns an `Ast.IfExpression` node.
  </description>
</function>

<function>
  <name>is_pattern_start?/1</name>
  <old_code>
defp is_pattern_start?(tokens) do
    case tokens do
      # Empty tokens can't start a pattern
      [] -> false
      # Number literals often start patterns
      [%Token{type: :number} | _] -> true
      # Identifiers often start patterns
      [%Token{type: :identifier} | _] -> true
      # Delimiters like (, [, { can start patterns
      [%Token{type: :delimiter, value: v} | _] when v in ["(", "[", "{"] -> true
      # String and char literals can be patterns
      [%Token{type: :string} | _] -> true
      [%Token{type: :char} | _] -> true
      # Keywords that might terminate a case expression
      [%Token{type: :keyword, value: val} | _] when val in ["end", "in", "else", "then"] -> true
      # Otherwise it's not a pattern start
      _ -> false
    end
  end
  </old_code>
  <description>
Helper function to heuristically check if a given list of tokens likely starts a new pattern.
It checks the type (and sometimes value) of the first token.
Returns `true` if the first token is a number, identifier, opening delimiter (`(`, `[`, `{`), string, char, or certain keywords that might indicate the end of a previous structure and start of a new one (though using keywords like "end", "in", "else", "then" as pattern starts seems unusual and might be more related to lookahead for termination of constructs like case expressions).
Returns `false` otherwise or if tokens are empty.
This function is not directly used in the provided snippet but might be intended for more complex layout-sensitive parsing or error recovery.
  </description>
</function>

<function>
  <name>parse_case_expression/1</name>
  <old_code>
def parse_case_expression(tokens) do
    with {:ok, _, tokens} <- expect_keyword(tokens, "case"),
         {:ok, expr, tokens} <- parse_expression(tokens),
         # ⭐ new
         tokens = skip_newlines(tokens),
         {:ok, _, tokens} <- expect_keyword(tokens, "of") do
      parse_case_clauses(tokens, expr, [])
    else
      other -> other
    end
  end
  </old_code>
  <description>
Parses a case expression, e.g., `case expr of pattern1 -> result1; pattern2 -> result2 end`.
It expects the "case" keyword, an expression to match against, the "of" keyword (with optional newlines in between), and then delegates to `parse_case_clauses/3` to parse the individual clauses.
  </description>
</function>

<function>
  <name>parse_case_clauses/3</name>
  <old_code>
def parse_case_clauses(tokens, expression, acc) do
    # Try to parse a single case clause
    case IO.inspect(parse_case_clause(tokens)) do
      {:ok, clause, remaining} ->
        # Successfully parsed a clause, continue with the remaining tokens
        parse_case_clauses(remaining, expression, [clause | acc])

      {:error, _} when acc != [] ->
        # If parsing fails but we've already got some clauses, we're done
        {:ok, %Ast.CaseExpression{expression: expression, cases: Enum.reverse(acc)}, tokens}

      {:error, reason} ->
        # If parsing fails and we have no clauses, propagate the error
        {:error, reason}
    end
  end
  </old_code>
  <description>
Recursively parses a sequence of case clauses for a case expression.
It attempts to parse a single clause using `parse_case_clause/1`.
If successful, it adds the clause to an accumulator and recurses with the remaining tokens.
If `parse_case_clause/1` fails but at least one clause has already been parsed, it considers the list of clauses complete and returns an `Ast.CaseExpression` node with the accumulated clauses. The `tokens` returned in this success case are the tokens *before* the failed `parse_case_clause` attempt, which seems correct as it signifies the end of parsable clauses.
If `parse_case_clause/1` fails and no clauses have been accumulated, it propagates the error.
The `IO.inspect` call is for debugging.
  </description>
</function>

<function>
  <name>split_until_newline/1</name>
  <old_code>
defp split_until_newline(tokens) do
    Enum.split_while(tokens, fn
      %Token{type: :newline} -> false
      _ -> true
    end)
  end
  </old_code>
  <description>
Splits a list of tokens into two parts: tokens up to (but not including) the first newline, and the rest of the tokens (starting with the newline, if present).
If no newline is found, the first part will be all tokens, and the second part will be empty.
This function is defined but not used in the provided snippet.
  </description>
</function>

<function>
  <name>clause_start?/1</name>
  <old_code>
def clause_start?(tokens) do
    # "pattern  ->"  without consuming the tokens
    with {:ok, _pat, rest} <- parse_pattern(tokens),
         {_, _guard, rest} <- maybe_parse_guard(rest),
         {:ok, _arrow, _} <- expect_operator(rest, "->") do
      true
    else
      _ -> false
    end
  end
  </old_code>
  <description>
Checks if the given tokens can start a new case clause (pattern, optional guard, arrow).
It attempts to parse a pattern, then an optional guard, and then an arrow `->` operator, all without permanently consuming tokens (as these are trial parses).
Returns `true` if this structure is found, `false` otherwise. Used for layout-sensitive parsing in `take_body`.
  </description>
</function>

<function>
  <name>take_body/3</name>
  <old_code>
defp take_body(tokens, acc, indent) do
    case tokens do
      [] ->
        {Enum.reverse(acc), []}

      [%Token{type: :newline} = nl | rest] ->
        rest = skip_newlines(rest)

        clause_start = clause_start?(rest)

        case rest do
          # ❶ plain dedent – always end the body
          [%Token{column: col} | _] = next when col < indent ->
            {Enum.reverse(acc), next}

          # ❷ same-column clause start – also end the body
          [%Token{column: col} | _] = next when col == indent and clause_start ->
            {Enum.reverse(acc), next}

          # ❸ otherwise keep accumulating
          _ ->
            take_body(rest, [nl | acc], indent)
        end

      [tok | rest] ->
        take_body(rest, [tok | acc], indent)
    end
  end
  </old_code>
  <description>
Collects tokens for the body of a layout-sensitive construct, like a case clause.
It accumulates tokens until it encounters a termination condition based on indentation and whether the next line starts a new clause.
- `tokens`: The remaining tokens to process.
- `acc`: Accumulated tokens for the current body.
- `indent`: The starting indentation level of the current construct (e.g., the pattern of a case clause).

Termination conditions (checked after a newline):
1.  Plain dedent: If the next token's column is less than `indent`.
2.  Same-column clause start: If the next token's column is equal to `indent` AND it looks like the start of a new clause (checked by `clause_start?/1`).
If neither condition is met, it continues accumulating tokens.
Returns `{body_tokens, remaining_tokens_after_body}`.
  </description>
</function>

<function>
  <name>parse_case_clause/1</name>
  <old_code>
def parse_case_clause(tokens) do
    tokens = skip_newlines(tokens)

    # Record the left-edge column of this clause’s pattern
    case tokens do
      [] ->
        {:error, "no more to parse"}

      [%Token{column: indent} | _] ->
        with {:ok, pattern, tokens} <- IO.inspect(parse_pattern(tokens)),
             {_, guard, tokens} <- IO.inspect(maybe_parse_guard(tokens)),
             {:ok, _, tokens} <- expect_operator(tokens, "->") do
          {body_tokens, rest} = take_body(tokens, [], indent)

          with {:ok, body, remaining} <- parse_expression(body_tokens),
               [] <- skip_newlines(remaining) do
            {:ok, %Ast.CaseClause{pattern: pattern, guard: guard, body: body},
             drop_newlines(rest)}
          else
            {:error, reason} -> {:error, reason}
            _ -> {:error, "unexpected tokens after case-clause body"}
          end
        end
    end
  end
  </old_code>
  <description>
Parses a single clause within a case expression, e.g., `pattern | guard -> body`.
It's layout-sensitive for the body.
1. Skips leading newlines.
2. Records the indentation of the pattern.
3. Parses the pattern (`parse_pattern/1`).
4. Parses an optional guard (`maybe_parse_guard/1`).
5. Expects an arrow `->` operator.
6. Uses `take_body/3` to collect tokens for the clause's body, respecting layout rules based on the pattern's indent.
7. Parses the collected body tokens as an expression.
8. Ensures no unexpected tokens are left within the parsed body tokens (after skipping newlines).
Returns an `Ast.CaseClause` node.
The `IO.inspect` calls are for debugging.
  </description>
</function>

<function>
  <name>maybe_parse_guard/1 (first clause)</name>
  <old_code>
def maybe_parse_guard([%Token{type: :operator, value: "|"} | rest] = all) do
    case IO.inspect(parse_expression(rest)) do
      {:ok, guard_ast, rest} ->
        {:ok, guard_ast, rest}

      {:error, _} ->
        {:error, nil, all}
    end
  end
  </old_code>
  <description>
Attempts to parse an optional guard expression in a case clause, e.g., `| x > 0`.
If the tokens start with a `|` operator, it tries to parse the following tokens as an expression.
If successful, returns `{:ok, guard_ast, remaining_tokens}`.
If parsing the expression fails after `|`, it returns `{:error, nil, original_tokens_including_pipe}` to indicate a guard was attempted but failed.
The `IO.inspect` call is for debugging.
  </description>
</function>

<function>
  <name>maybe_parse_guard/1 (second clause)</name>
  <old_code>
def maybe_parse_guard(tokens), do: {:error, nil, tokens}
  </old_code>
  <description>
Handles the case where there is no `|` operator at the beginning of the tokens, meaning no guard is present.
Returns `{:error, nil, original_tokens}`, indicating no guard was parsed. The "error" status here signifies absence rather than a parsing failure of an attempted guard.
  </description>
</function>

<function>
  <name>parse_do_block/1</name>
  <old_code>
defp parse_do_block(tokens) do
    with {:ok, _, tokens} <- expect_keyword(tokens, "do"),
         {:ok, expressions, tokens} <- parse_many(&parse_do_expression/1, tokens),
         remaining = skip_until_end(tokens) do
      {:ok, %Ast.DoBlock{expressions: expressions}, remaining}
    else
      {:error, reason} -> {:error, reason}
    end
  end
  </old_code>
  <description>
Parses a do-block, e.g., `do let x = 1; y <- M.action; pure (x+y) end`.
It expects the "do" keyword, followed by a sequence of "do expressions" (parsed by `parse_many(&parse_do_expression/1, ...)`).
After parsing the expressions, it calls `skip_until_end/1` to consume tokens up to and including the "end" keyword (if present and if the language uses "end" for do-blocks).
Returns an `Ast.DoBlock` node.
  </description>
</function>

<function>
  <name>parse_do_expression/1</name>
  <old_code>
defp parse_do_expression(tokens) do
    parse_any(
      [
        fn tokens ->
          with {:ok, _, tokens} <- expect_keyword(tokens, "let"),
               {:ok, name, tokens} <- parse_identifier(tokens),
               {:ok, _, tokens} <- expect_operator(tokens, "="),
               {:ok, value, tokens} <- parse_expression(tokens) do
            {:ok, {:let, name.name, value}, tokens}
          else
            {:error, _} -> {:error, "Expected let binding in do block"}
          end
        end,
        fn tokens ->
          with {:ok, expr, tokens} <- parse_expression(tokens),
               {:ok, _, tokens} <- expect_operator(tokens, "<-"),
               {:ok, value, tokens} <- parse_expression(tokens) do
            {:ok, {:bind, expr, value}, tokens}
          else
            {:error, _} -> {:error, "Expected bind expression in do block"}
          end
        end,
        fn tokens ->
          with {:ok, expr, tokens} <- parse_expression(tokens) do
            {:ok, {:expr, expr}, tokens}
          else
            {:error, _} -> {:error, "Expected expression in do block"}
          end
        end
      ],
      tokens
    )
  end
  </old_code>
  <description>
Parses a single statement or expression within a do-block. It uses `parse_any` to try:
1.  A let binding: `let name = value`, returns `{:let, name_string, value_ast}`.
2.  A bind expression: `pattern <- monadic_value` (Note: current code parses `expr <- value`, `expr` should probably be `parse_pattern`), returns `{:bind, pattern_ast, value_ast}`.
3.  A simple expression: `expr`, returns `{:expr, expr_ast}`.
The order matters: `let` is more specific. Bind vs. simple expression order depends on whether `<-` is common or expressions are.
  </description>
</function>

<function>
  <name>parse_binary_expression/1</name>
  <old_code>
defp parse_binary_expression(tokens) do
    parse_any(
      [
        &parse_let_expression/1,
        &parse_if_expression/1,
        &parse_case_expression/1,
        &parse_do_block/1,
        &parse_lambda/1,
        &parse_dollar_expression/1
      ],
      tokens
    )
  end
  </old_code>
  <description>
Serves as an entry point for parsing expressions that are either non-binary constructs or the start of a precedence climbing chain for binary operators.
It uses `parse_any` to try parsing various "block" or "atomic" expression forms first: let, if, case, do-block, lambda.
If none of those match, it attempts to parse a `dollar_expression` (which handles the `$` operator and then falls through to other binary operators via `parse_logical_expression`).
This function effectively sets up the highest level of expression parsing before diving into operator precedence.
  </description>
</function>

<function>
  <name>parse_dollar_expression/1</name>
  <old_code>
defp parse_dollar_expression(tokens) do
    # first parse *anything* tighter than '$'
    with {:ok, left, tokens} <- parse_logical_expression(tokens) do
      tokens = skip_newlines(tokens)

      case tokens do
        [%Token{type: :operator, value: "$"} | rest] ->
          rest = skip_newlines(rest)

          # right-associative: parse the *whole* rhs with the same rule
          with {:ok, right, rest} <- parse_dollar_expression(rest) do
            {:ok, %Ast.FunctionCall{function: left, arguments: [right]}, rest}
          end

        _ ->
          {:ok, left, tokens}
      end
    end
  end
  </old_code>
  <description>
Parses expressions involving the `$` operator (function application with low, right-associative precedence).
It first parses the left-hand side using `parse_logical_expression` (which has higher precedence).
If a `$` operator is found, it recursively calls `parse_dollar_expression` for the right-hand side (to handle right-associativity like `f $ g $ x`).
The `$` application `f $ x` is represented as `Ast.FunctionCall{function: f, arguments: [x]}`.
If no `$` is found, it returns the parsed left-hand side.
  </description>
</function>

<function>
  <name>parse_logical_expression/1</name>
  <old_code>
defp parse_logical_expression(tokens) do
    with {:ok, left, tokens} <- parse_comparison_expression(tokens) do
      case tokens do
        [%Token{type: :operator, value: op} | rest] when op in ["&&", "||"] ->
          with {:ok, right, rest} <- parse_logical_expression(rest) do
            {:ok, %Ast.BinaryOp{op: op, left: left, right: right}, rest}
          end

        _ ->
          {:ok, left, tokens}
      end
    end
  end
  </old_code>
  <description>
Parses logical AND (`&&`) and OR (`||`) expressions. These are typically left-associative or handled with specific precedence rules. This implementation appears right-associative due to the recursive call `parse_logical_expression(rest)` for the right operand.
It first parses the left-hand side using `parse_comparison_expression` (which has higher precedence).
If a `&&` or `||` operator is found, it parses the right-hand side and constructs an `Ast.BinaryOp` node.
If no such operator is found, it returns the parsed left-hand side.
  </description>
</function>

<function>
  <name>parse_comparison_expression/1</name>
  <old_code>
defp parse_comparison_expression(tokens) do
    tokens = skip_newlines(tokens)

    with {:ok, left, tokens} <- parse_additive_expression(tokens) do
      tokens = skip_newlines(tokens)

      case tokens do
        [%Token{type: :operator, value: op} | rest]
        when op in ["==", "!=", "<", "<=", ">", ">="] ->
          rest = skip_newlines(rest)

          with {:ok, right, rest} <- parse_comparison_expression(rest) do
            {:ok, %Ast.BinaryOp{op: op, left: left, right: right}, rest}
          end

        _ ->
          {:ok, left, tokens}
      end
    end
  end
  </old_code>
  <description>
Parses comparison expressions (e.g., `==`, `!=`, `<`, `<=`, `>`, `>=`). These are typically non-associative or left-associative. This implementation appears right-associative due to the recursive call `parse_comparison_expression(rest)` for the right operand.
It first parses the left-hand side using `parse_additive_expression` (which has higher precedence).
If a comparison operator is found, it parses the right-hand side and constructs an `Ast.BinaryOp` node.
If no such operator is found, it returns the parsed left-hand side.
Skips newlines before the operator and its right operand.
  </description>
</function>

<function>
  <name>parse_additive_expression/1</name>
  <old_code>
defp parse_additive_expression(tokens) do
    with {:ok, left, tokens} <- parse_multiplicative_expression(tokens) do
      case tokens do
        [%Token{type: :operator, value: op} | rest] when op in ["+", "-", "++", "<>"] ->
          with {:ok, right, rest} <- parse_additive_expression(rest) do
            {:ok, %Ast.BinaryOp{op: op, left: left, right: right}, rest}
          end

        _ ->
          {:ok, left, tokens}
      end
    end
  end
  </old_code>
  <description>
Parses additive expressions (`+`, `-`) and potentially string/list concatenation (`++`, `<>`). These are typically left-associative. This implementation appears right-associative due to the recursive call `parse_additive_expression(rest)` for the right operand.
It first parses the left-hand side using `parse_multiplicative_expression` (which has higher precedence).
If an additive operator is found, it parses the right-hand side and constructs an `Ast.BinaryOp` node.
If no such operator is found, it returns the parsed left-hand side.
  </description>
</function>

<function>
  <name>parse_record_literal/1 (first clause)</name>
  <old_code>
defp parse_record_literal([%Token{type: :delimiter, value: "{"} | rest]) do
    with {:ok, fields, rest} <-
           parse_separated(&parse_record_field_expr/1, &expect_delimiter(&1, ","), rest),
         {:ok, _, rest} <- expect_delimiter(rest, "}") do
      {:ok, %Ast.RecordLiteral{fields: fields}, rest}
    end
  end
  </old_code>
  <description>
Parses a record literal, e.g., `{ field1: expr1, field2: expr2 }`.
It expects an opening brace `{`, a comma-separated list of record field expressions (parsed by `parse_record_field_expr/1`), and a closing brace `}`.
Returns an `Ast.RecordLiteral` node.
  </description>
</function>

<function>
  <name>parse_record_literal/1 (second clause)</name>
  <old_code>
defp parse_record_literal(_), do: {:error, "Expected record literal"}
  </old_code>
  <description>
Handles the case where the input tokens do not start with an opening brace, indicating it's not a record literal.
  </description>
</function>

<function>
  <name>parse_record_field_expr/1</name>
  <old_code>
defp parse_record_field_expr(tokens) do
    with {:ok, label, tokens} <- parse_identifier(tokens),
         # uses the new ':'
         {:ok, _, tokens} <- expect_delimiter(tokens, ":"),
         tokens = skip_newlines(tokens),
         {:ok, expr, tokens} <- parse_expression(tokens) do
      {:ok, {label.name, expr}, tokens}
    end
  end
  </old_code>
  <description>
Parses a single field within a record literal, e.g., `fieldName: expression`.
It expects an identifier for the label (field name), a colon `:` delimiter, and then an expression.
Skips newlines after the colon.
Returns a tuple `{field_name_string, expression_ast}`.
  </description>
</function>

<function>
  <name>parse_multiplicative_expression/1</name>
  <old_code>
defp parse_multiplicative_expression(tokens) do
    # <- was parse_comparison_expression/1
    with {:ok, left, tokens} <- parse_application(tokens) do
      case tokens do
        [%Token{type: :operator, value: op} | rest] when op in ["*", "/"] ->
          with {:ok, right, rest} <- parse_multiplicative_expression(rest) do
            {:ok, %Ast.BinaryOp{op: op, left: left, right: right}, rest}
          end

        [
          %Token{type: :operator, value: "`"},
          %Token{type: :identifier, value: fun},
          %Token{type: :operator, value: "`"} | rest
        ] ->
          with {:ok, right, rest} <- parse_multiplicative_expression(rest) do
            {:ok,
             %Ast.FunctionCall{
               function: %Ast.Identifier{name: fun},
               arguments: [left, right]
             }, rest}
          end

        _ ->
          {:ok, left, tokens}
      end
    end
  end
  </old_code>
  <description>
Parses multiplicative expressions (`*`, `/`) and infix function calls using backticks (e.g., ``arg1 `func` arg2``). These are typically left-associative. This implementation appears right-associative for `*` and `/` due to the recursive call `parse_multiplicative_expression(rest)` for the right operand. Infix function calls also seem to be parsed right-associatively.
It first parses the left-hand side using `parse_application` (which has higher precedence).
If a `*` or `/` operator is found, it parses the right-hand side and constructs an `Ast.BinaryOp`.
If an infix function call (`` `ident` ``) is found, it parses the right-hand side and constructs an `Ast.FunctionCall`.
If no such operator/construct is found, it returns the parsed left-hand side.
  </description>
</function>

<function>
  <name>parse_application/1 (main clause)</name>
  <old_code>
defp parse_application([%Token{column: base} | _] = toks) do
    with {:ok, fn_term, rest} <- parse_term(toks) do
      {args, rest} = collect_application_args(rest, [], base)

      case args do
        [] -> {:ok, fn_term, rest}
        _ -> {:ok, %Ast.FunctionCall{function: fn_term, arguments: args}, rest}
      end
    end
  end
  </old_code>
  <description>
Parses function application expressions, e.g., `f x y`. Function application is left-associative.
It first parses a "term" (e.g., an identifier, literal, parenthesized expression) which serves as the function.
Then, it calls `collect_application_args/3` to gather subsequent terms as arguments. This helper handles layout sensitivity for arguments on new lines.
If arguments are found, it constructs an `Ast.FunctionCall` node. Otherwise, it returns the initial term.
The `base` column is used by `collect_application_args` for layout decisions.
  </description>
</function>

<function>
  <name>parse_application/1 (empty tokens clause)</name>
  <old_code>
defp parse_application([]) do
    {:error, :no_tokens_remaining}
  end
  </old_code>
  <description>
Handles the case where `parse_application` is called with an empty list of tokens, returning an error.
  </description>
</function>

<function>
  <name>collect_application_args/3 (newline handling clause)</name>
  <old_code>
defp collect_application_args([%Token{type: :newline} | rest], acc, base) do
    rest = skip_newlines(rest)

    case rest do
      [%Token{column: col} | _] when col > base ->
        case parse_term(rest) do
          {:ok, arg, rest2} -> collect_application_args(rest2, acc ++ [arg], base)
          # something else - abort
          {:error, _} -> {acc, rest}
        end

      _ ->
        # dedent or EOF → application ends
        {acc, rest}
    end
  end
  </old_code>
  <description>
Helper for `parse_application/1` to collect arguments, specifically handling arguments on new lines.
- `tokens`: Starts with a newline.
- `acc`: Accumulated arguments.
- `base`: Indentation column of the function term.
It skips subsequent newlines. If the next token is indented further than `base`, it attempts to parse another argument term. If successful, it recurses.
If the next token is not indented further (dedent or same column), or if parsing a term fails, it stops collecting arguments and returns the accumulated arguments and the current remaining tokens.
  </description>
</function>

<function>
  <name>collect_application_args/3 (general clause)</name>
  <old_code>
defp collect_application_args(tokens, acc, base) do
    case parse_term(tokens) do
      {:ok, arg, rest} -> collect_application_args(rest, acc ++ [arg], base)
      {:error, _} -> {acc, tokens}
    end
  end
  </old_code>
  <description>
Helper for `parse_application/1` to collect arguments that are on the same line as the function or previous argument.
- `tokens`: Remaining tokens.
- `acc`: Accumulated arguments.
- `base`: Indentation column (used by the newline-handling clause, not directly here but passed along).
It attempts to parse an argument term. If successful, it adds the argument to the accumulator and recurses with the rest of the tokens.
If parsing a term fails, it means no more arguments on this line, so it returns the accumulated arguments and the current tokens.
  </description>
</function>

<function>
  <name>parse_term/1</name>
  <old_code>
defp parse_term(tokens) do
    parse_any(
      [
        &parse_record_literal/1,
        &parse_literal/1,
        &parse_list_literal/1,
        &parse_list_comprehension/1,
        &parse_tuple_literal/1,
        &parse_qualified_identifier/1,
        fn tokens ->
          case tokens do
            [%Token{type: :delimiter, value: "("} | rest] ->
              with {:ok, expr, rest} <- parse_expression(rest),
                   {:ok, _, rest} <- expect_delimiter(rest, ")") do
                {:ok, expr, rest}
              end

            _ ->
              {:error, "Expected parenthesized expression"}
          end
        end
      ],
      tokens
    )
  end
  </old_code>
  <description>
Parses a "term," which is an atomic unit of an expression, often an operand or the base of a function application.
It uses `parse_any` to try parsing:
a record literal, a literal (number, string, char), a list literal, a list comprehension, a tuple literal, a qualified identifier, or a parenthesized expression (which recursively calls `parse_expression` for the inner content).
The order reflects precedence or specificity (e.g., record literal `{...}` before qualified identifier which might start with `{` in some languages if identifiers could be keywords).
  </description>
</function>

<function>
  <name>parse_list_literal/1</name>
  <old_code>
defp parse_list_literal(tokens) do
    case tokens do
      [%Token{type: :delimiter, value: "["} | rest] ->
        with {:ok, elements, rest} <-
               parse_separated(&parse_expression/1, &expect_delimiter(&1, ","), rest),
             {:ok, _, rest} <- expect_delimiter(rest, "]") do
          {:ok, %Ast.List{elements: elements}, rest}
        end

      _ ->
        {:error, "Expected list literal"}
    end
  end
  </old_code>
  <description>
Parses a list literal, e.g., `[1, 2, 3]` or `[]`.
It expects an opening square bracket `[`, a comma-separated list of expressions (parsed by `parse_expression/1`), and a closing square bracket `]`.
Returns an `Ast.List` node.
  </description>
</function>

<function>
  <name>parse_list_comprehension/1</name>
  <old_code>
defp parse_list_comprehension(tokens) do
    case tokens do
      [%Token{type: :delimiter, value: "["} | rest] ->
        with {:ok, expression, rest} <- parse_expression(rest),
             {:ok, _, rest} <- expect_operator(rest, "|"),
             {:ok, generators, rest} <-
               parse_separated(&parse_generator/1, &expect_delimiter(&1, ","), rest),
             {:ok, _, rest} <- expect_delimiter(rest, "]") do
          {:ok,
           %Ast.ListComprehension{expression: expression, generators: generators, guards: []},
           rest}
        end

      _ ->
        {:error, "Expected list comprehension"}
    end
  end
  </old_code>
  <description>
Parses a list comprehension, e.g., `[x * 2 | x <- myList, x > 0]`.
It expects an opening square bracket `[`, an expression for the output element, a pipe `|` operator, a comma-separated list of generators (and potentially filters, though filters/guards are not explicitly parsed here beyond being part of a generator's syntax if `parse_generator` handled them), and a closing square bracket `]`.
The `guards` field in `Ast.ListComprehension` is initialized to an empty list, suggesting guards might be parsed within `parse_generator` or are a planned feature.
Returns an `Ast.ListComprehension` node.
  </description>
</function>

<function>
  <name>parse_generator/1</name>
  <old_code>
defp parse_generator(tokens) do
    with {:ok, pattern, tokens} <- parse_pattern(tokens),
         {:ok, _, tokens} <- expect_operator(tokens, "<-"),
         {:ok, expression, tokens} <- parse_expression(tokens) do
      {:ok, %Ast.Generator{pattern: pattern, expression: expression}, tokens}
    else
      {:error, reason} -> {:error, reason}
    end
  end
  </old_code>
  <description>
Parses a generator clause within a list comprehension or do-block, e.g., `pattern <- expression`.
It expects a pattern (parsed by `parse_pattern/1`), a left arrow `<-` operator, and an expression that yields the collection to iterate over.
Returns an `Ast.Generator` node. This could also parse boolean expressions (filters/guards) if the grammar allowed `expression` here to be a boolean condition not preceded by `pattern <-`.
  </description>
</function>

<function>
  <name>parse_tuple_literal/1</name>
  <old_code>
defp parse_tuple_literal(tokens) do
    case tokens do
      [%Token{type: :delimiter, value: "("} | rest] ->
        with {:ok, elements, rest} <-
               parse_separated(&parse_expression/1, &expect_delimiter(&1, ","), rest),
             {:ok, _, rest} <- expect_delimiter(rest, ")") do
          if length(elements) == 1 do
            # Single element in parentheses is just grouping, not a tuple
            {:ok, List.first(elements), rest}
          else
            {:ok, %Ast.Tuple{elements: elements}, rest}
          end
        end

      _ ->
        {:error, "Expected tuple literal"}
    end
  end
  </old_code>
  <description>
Parses a tuple literal, e.g., `(1, "hello")` or `(true)`.
It expects an opening parenthesis `(`, a comma-separated list of expressions (parsed by `parse_expression/1`), and a closing parenthesis `)`.
If there's only one element inside the parentheses (e.g., `(expr)`), it's treated as a grouped expression, and the inner expression AST is returned directly.
Otherwise (0 or 2+ elements), it returns an `Ast.Tuple` node.
  </description>
</function>

<function>
  <name>parse_literal/1</name>
  <old_code>
defp parse_literal(tokens) do
    tokens = skip_newlines(tokens)

    case tokens do
      [%Token{type: :number, value: value} | rest] ->
        {:ok, %Ast.Literal{type: :number, value: value}, rest}

      [%Token{type: :string, value: value} | rest] ->
        {:ok, %Ast.Literal{type: :string, value: value}, rest}

      [%Token{type: :char, value: value} | rest] ->
        {:ok, %Ast.Literal{type: :char, value: value}, rest}

      _ ->
        {:error, "Expected literal"}
    end
  end
  </old_code>
  <description>
Parses a literal value: number, string, or character.
It skips leading newlines. Then, it checks the type of the first token.
If it's a `:number`, `:string`, or `:char` token, it creates an `Ast.Literal` node with the corresponding type and value.
  </description>
</function>

<function>
  <name>parse_string_literal/1</name>
  <old_code>
defp parse_string_literal(tokens) do
    tokens = skip_newlines(tokens)

    case tokens do
      [%Token{type: :string, value: value} | rest] -> {:ok, value, rest}
      _ -> {:error, "Expected string literal"}
    end
  end
  </old_code>
  <description>
Parses a string literal token and returns its string value directly, not an AST node.
It skips leading newlines and expects a token of type `:string`.
Used, for example, in `parse_foreign_import` for foreign module/function names.
  </description>
</function>

<function>
  <name>parse_label/1</name>
  <old_code>
defp parse_label(tokens) do
    tokens = skip_newlines(tokens)

    case tokens do
      [%Token{type: t, value: name} | rest] when t in [:identifier, :keyword] ->
        {:ok, %Ast.Identifier{name: name}, rest}

      _ ->
        {:error, "Expected label"}
    end
  end
  </old_code>
  <description>
Parses a label, which is typically an identifier used as a field name in records or objects.
It skips leading newlines. It expects a token that is either an `:identifier` or a `:keyword` (allowing keywords to be used as labels).
Returns an `Ast.Identifier` node containing the label's name.
  </description>
</function>

<function>
  <name>parse_identifier/1</name>
  <old_code>
defp parse_identifier(tokens) do
    tokens = skip_newlines(tokens)

    case tokens do
      [%Token{type: :identifier, value: name} | rest] -> {:ok, %Ast.Identifier{name: name}, rest}
      _ -> {:error, "Expected identifier"}
    end
  end
  </old_code>
  <description>
Parses a simple identifier.
It skips leading newlines and expects a token of type `:identifier`.
Returns an `Ast.Identifier` node.
  </description>
</function>

<function>
  <name>parse_qualified_identifier/1</name>
  <old_code>
defp parse_qualified_identifier(tokens) do
    parse_separated(&parse_identifier/1, &expect_operator(&1, "."), tokens)
    |> case do
      {:ok, parts, rest} when is_list(parts) ->
        {:ok, %Ast.Identifier{name: Enum.map(parts, & &1.name) |> Enum.join(".")}, rest}

      other ->
        other
    end
  end
  </old_code>
  <description>
Parses a qualified identifier, e.g., `Module.SubModule.name`.
It uses `parse_separated` to parse one or more simple identifiers (parsed by `parse_identifier/1`) separated by dot `.` operators.
If successful, it joins the names of the identifier parts with dots to form a single string and returns this in an `Ast.Identifier` node.
A single, non-qualified identifier will also be parsed successfully by this function (as a list with one part).
  </description>
</function>

<function>
  <name>parse_any/2</name>
  <old_code>
defp parse_any(parsers, tokens) do
    case parsers do
      [] ->
        {:error, "No parser succeeded #{inspect(List.first(tokens))}"}

      [parser | rest] ->
        case parser.(tokens) do
          {:ok, result, remaining} -> {:ok, result, remaining}
          {:error, _} -> parse_any(rest, tokens)
        end
    end
  end
  </old_code>
  <description>
A parser combinator that tries a list of parser functions one by one on the same input tokens.
It takes a list of `parser` functions and an initial `tokens` list.
It applies the first parser. If it succeeds (`{:ok, ...}`), `parse_any` returns that result.
If it fails (`{:error, ...}`), it tries the next parser in the list with the original `tokens`.
If all parsers in the list fail, it returns an error indicating that no parser succeeded, including the first token for debugging.
  </description>
</function>

<function>
  <name>parse_many/2 (public arity 2)</name>
  <old_code>
def parse_many(parser, tokens) do
    parse_many(parser, tokens, [])
  end
  </old_code>
  <description>
Public entry point for `parse_many/3`. It initializes an empty accumulator for the results.
This function attempts to apply a given `parser` function zero or more times consecutively.
  </description>
</function>

<function>
  <name>parse_many/3 (recursive step)</name>
  <old_code>
def parse_many(parser, tokens, acc) do
    case parser.(tokens) do
      {:ok, result, remaining} ->
        parse_many(parser, remaining, [result | acc])

      {:error, _} ->
        {:ok, Enum.reverse(acc), tokens}
    end
  end
  </old_code>
  <description>
A parser combinator that applies a given `parser` function repeatedly to consume a sequence of elements.
It takes a `parser` function, the current `tokens`, and an `acc`umulator for parsed results.
It calls `parser.(tokens)`.
If the `parser` succeeds, it adds the `result` to the `acc` and recursively calls `parse_many` with the `remaining` tokens.
If the `parser` fails, it means no more elements of that kind can be parsed. It then returns `{:ok, Enum.reverse(acc), tokens}`, where `acc` contains all successfully parsed elements in order, and `tokens` are the tokens that caused the last `parser` call to fail (i.e., the tokens remaining after the last successful parse).
This effectively parses zero or more occurrences.
  </description>
</function>

<function>
  <name>parse_separated/3</name>
  <old_code>
defp parse_separated(parser, separator, tokens) do
    with {:ok, first, tokens} <- parser.(tokens) do
      parse_separated_rest(parser, separator, tokens, [first])
    else
      {:error, reason} -> {:error, reason}
    end
  end
  </old_code>
  <description>
A parser combinator for parsing one or more occurrences of an element (parsed by `parser`) separated by a `separator` (parsed by `separator_parser`).
Example: `item1, item2, item3`.
It first tries to parse one `item` using `parser`. If this fails, `parse_separated` fails.
If the first item is parsed successfully, it calls `parse_separated_rest/4` to handle subsequent items and separators.
This parser expects at least one item.
  </description>
</function>

<function>
  <name>parse_separated_rest/4</name>
  <old_code>
defp parse_separated_rest(parser, separator, tokens, acc) do
    case separator.(tokens) do
      {:ok, _, tokens} ->
        case parser.(tokens) do
          {:ok, item, rest} ->
            parse_separated_rest(parser, separator, rest, [item | acc])

          {:error, _} ->
            {:error, "Expected item after separator"}
        end

      {:error, _} ->
        # No more separators, we're done
        {:ok, Enum.reverse(acc), tokens}
    end
  end
  </old_code>
  <description>
Helper function for `parse_separated/3`. It attempts to parse a `separator` followed by another `item`.
- `parser`: Function to parse an item.
- `separator`: Function to parse the separator.
- `tokens`: Current list of tokens.
- `acc`: Accumulated list of parsed items.

It first tries to parse a `separator`.
If successful, it then *must* parse another `item` using `parser`. If this item parsing fails, it's an error ("Expected item after separator"). If successful, it recurses.
If parsing the `separator` fails, it means the list of separated items has ended. It returns the accumulated items (reversed for correct order) and the current tokens (which start with whatever caused the separator parse to fail).
  </description>
</function>

<function>
  <name>expect_keyword/2</name>
  <old_code>
def expect_keyword(tokens, expected) do
    tokens = skip_newlines(tokens)

    case tokens do
      [%Token{type: :keyword, value: ^expected} | rest] ->
        {:ok, expected, rest}

      _ ->
        {:error, "Expected keyword '#{expected}'"}
    end
  end
  </old_code>
  <description>
Expects and consumes a specific keyword token.
It skips leading newlines. Then, it checks if the first token is of type `:keyword` and its `value` matches the `expected` keyword string.
If it matches, returns `{:ok, expected_keyword_string, remaining_tokens}`.
Otherwise, returns an error.
  </description>
</function>

<function>
  <name>expect_operator/2</name>
  <old_code>
defp expect_operator(tokens, expected) do
    tokens = skip_newlines(tokens)

    case tokens do
      [%Token{type: :operator, value: ^expected} | rest] ->
        {:ok, expected, rest}

      _ ->
        {:error, "Expected operator '#{expected}'"}
    end
  end
  </old_code>
  <description>
Expects and consumes a specific operator token.
It skips leading newlines. Then, it checks if the first token is of type `:operator` and its `value` matches the `expected` operator string.
If it matches, returns `{:ok, expected_operator_string, remaining_tokens}`.
Otherwise, returns an error.
  </description>
</function>

<function>
  <name>expect_delimiter/2</name>
  <old_code>
defp expect_delimiter(tokens, expected) do
    tokens = skip_newlines(tokens)

    case tokens do
      [%Token{type: :delimiter, value: ^expected} | rest] ->
        {:ok, expected, rest}

      _ ->
        {:error, "Expected delimiter '#{expected}'"}
    end
  end
  </old_code>
  <description>
Expects and consumes a specific delimiter token (e.g., '(', ')', '[', ']', '{', '}', ',').
It skips leading newlines. Then, it checks if the first token is of type `:delimiter` and its `value` matches the `expected` delimiter string.
If it matches, returns `{:ok, expected_delimiter_string, remaining_tokens}`.
Otherwise, returns an error.
  </description>
</function>

<function>
  <name>skip_until_end/1</name>
  <old_code>
defp skip_until_end(tokens) do
    case Enum.find_index(tokens, fn token -> token.type == :keyword and token.value == "end" end) do
      nil -> tokens
      idx -> Enum.drop(tokens, idx + 1)
    end
  end
  </old_code>
  <description>
Skips tokens until (and including) an "end" keyword is found.
It searches for the first token that is a `:keyword` with value `"end"`.
If found, it returns the list of tokens *after* this "end" keyword.
If not found, it returns the original list of tokens (meaning "end" was not present or already consumed).
This is used in `parse_do_block` to consume the trailing "end" of a do-block.
  </description>
</function>
